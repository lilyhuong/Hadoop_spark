{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF33cvquA22Y"
      },
      "source": [
        "# Mini-projet Apache Spark\n",
        "\n",
        "Merci d'indiquer les noms composant le bin√¥me :\n",
        "\n",
        "|Nom | Pr√©nom|\n",
        "|---|---|\n",
        "| A | compl√©ter |\n",
        "| A | compl√©ter |\n",
        "\n",
        "L'objectif est de collecter les donn√©es de validations quotidiennes de titres de transport de la r√©gion parisienne, disponibles en Open Data sur le site de Mobilit√©s Ile-de-France.\n",
        "\n",
        "On se concentrera sur le **r√©seau ferr√©** exclusivement pour cet exercice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fXsbY1BRCt"
      },
      "source": [
        "## Installation de Spark ‚ö°\n",
        "\n",
        "Apache Spark n'est pas disponible en standard sur Google Colab.\n",
        "Proc√©der √† son installation, ainsi qu'√† son initialisation pour r√©aliser le traitement √† venir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xk3QdgByAw-h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.9/site-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The operation couldn‚Äôt be completed. Unable to locate a Java Runtime.\n",
            "Please visit http://www.java.com for information on installing Java.\n",
            "\n",
            "/opt/anaconda3/lib/python3.9/site-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
            "head: illegal line count -- -1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Java gateway process exited before sending its port number",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/Users/lilyhuong/Desktop/Amse mag3/Hadoop_spark/Traitement_des_validations_du_r√©seau_de_transport_dIle_de_France_2022.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lilyhuong/Desktop/Amse%20mag3/Hadoop_spark/Traitement_des_validations_du_r%C3%A9seau_de_transport_dIle_de_France_2022.ipynb#ch0000031?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39;49msql\u001b[39m.\u001b[39;49mSparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mProject 2022\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=266'>267</a>\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=267'>268</a>\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=268'>269</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=269'>270</a>\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=270'>271</a>\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=271'>272</a>\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=480'>481</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=481'>482</a>\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=482'>483</a>\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=483'>484</a>\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=484'>485</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=188'>189</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=189'>190</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=190'>191</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=191'>192</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=192'>193</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=194'>195</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=195'>196</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=196'>197</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=197'>198</a>\u001b[0m         master,\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=198'>199</a>\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=207'>208</a>\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=208'>209</a>\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=414'>415</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=415'>416</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=416'>417</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=417'>418</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=419'>420</a>\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=102'>103</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=104'>105</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=105'>106</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=107'>108</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=108'>109</a>\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
          ]
        }
      ],
      "source": [
        "spark = pyspark.sql.SparkSession.builder.appName('Project 2022').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFkMo_2WBcrG"
      },
      "source": [
        "## R√©cup√©ration des donn√©es üöÜ\n",
        "\n",
        "Sur le site https://data.iledefrance-mobilites.fr, r√©cup√©rer les donn√©es de validation par jour.\n",
        "\n",
        "On r√©cup√©rera les fichiers des validations sur le r√©seau de surface disponibles ici [https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/table/](https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/table/). On prendra les ann√©es 2020 et 2021 pour l'exercice :\n",
        "* data-rf-2020.zip\n",
        "* data-rf-2021.zip\n",
        "\n",
        "Identifiez les donn√©es de S1 2020 √† S2 2021 pour disposer d'un historique de deux ans.\n",
        "\n",
        "On utilisera pour ce faire les commandes de t√©l√©chargement de fichiers (`curl`) depuis un site (pas de chargement manuel).\n",
        "\n",
        "__Attention__ : pr√©voir quelques minutes pour le t√©l√©chargement, au moins une premi√®re fois, et donc une copie sur Google Drive si Google Colab est utilis√©e, afin d'√©viter ce temps d'attente lors de sessions de travail successives.\n",
        "\n",
        "Les fichiers sont des zip. Ils peuvent √™tre d√©compress√©s avec la commande `unzip`.\n",
        "\n",
        "Ces fichiers seront localis√©s dans un sous-r√©pertoire.\n",
        "\n",
        "__Documentation de curl__ : http://pwet.fr/man/linux/commandes/curl/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6mVDDBi4PAWT"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JieIOSxp_8wM"
      },
      "source": [
        "### Commentaires sur les donn√©es disponibles sur ce portail\n",
        "\n",
        "Investiguer les donn√©es diponibles sur le portail.\n",
        "\n",
        "Question : peut-on constituer un historique de donn√©es s'√©tendant sur les trois derni√®res ann√©es (2019 √† 2021) ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEXg6TeoATXD"
      },
      "source": [
        "**Remplir votre commentaire ici**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i0jSKz1Orfl"
      },
      "source": [
        "## Lecture des fichiers dans Spark üìÅ\n",
        "\n",
        "Lire les fichiers en choisissant les bonnes options de lecture.\n",
        "Concat√©ner les donn√©es en une seule table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Me1V5NfYx2A"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wPinQtj8vm7"
      },
      "source": [
        "## Pr√©paration des donn√©es\n",
        "\n",
        "R√©aliser les transformations n√©cessaires pour exploiter ces donn√©es :\n",
        "- pr√©paration des dates (champ JOUR)\n",
        "\n",
        "Quelle est la tranformation √† r√©aliser ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRzuTMKgO3Uw"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDTdkHk8lae"
      },
      "source": [
        "## D√©termination des principales cat√©gories de titre\n",
        "\n",
        "Diff√©rentes cat√©gories de titre sont utilis√©es sur le r√©seau.\n",
        "\n",
        "D√©terminer les deux cat√©gories principalement utilis√©es. Seules ces cat√©gories seront utilis√©es dans les travaux ci-apr√®s (les utiliser comme filtre sur les validations dans la suite)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGvfkzZTFiLn"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iY3-EDD6QfK"
      },
      "source": [
        "## Visualisation du trafic dans une station\n",
        "\n",
        "Visualiser le trafic √† la gare de Lyon pour les deux cat√©gories de titre principales.\n",
        "\n",
        "Attention √† g√©rer le cas des gares (comme la gare de Lyon) pr√©sentes sur plusieurs lignes et dont le libell√© appara√Æt donc sur plusieurs lignes. Investiguer ce cas avant de d√©terminer la bonne fa√ßon de calculer le nombre de validations pour la gare de Lyon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7kJok808_0H"
      },
      "outputs": [],
      "source": [
        "# Votre code mettant en √©vidence le cas des gares sur plusieurs lignes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeOqG1Hi9EuM"
      },
      "outputs": [],
      "source": [
        "# Votre code visualisant le trafic √† la gare de Lyon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ae6sckJdxjE"
      },
      "source": [
        "## Fluctuation du trafic hebdomadaire\n",
        "\n",
        "Calculer le trafic total et le pourcentage par jour de la semaine sur l'ensemble du r√©seau.\n",
        "\n",
        "Trier le r√©sultat par ordre d√©croissant de validations.\n",
        "\n",
        "Note : consid√©rer l'usage d'une fonction analytique (`Window.partitionBy()`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku4EWiub-x4r"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvhcCQaJaB8i"
      },
      "source": [
        "### Impact du t√©l√©travail\n",
        "\n",
        "Mettre en √©vidence l'impact du t√©l√©travail en comparant les comportements hebdomadaires de janvier et f√©vrier 2020 et ceux de janvier et f√©vrier 2021."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqg42KydaQ7h"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMN5ZAs_dRWN"
      },
      "source": [
        "## Analyse de l'impact du reconfinement d'octobre 2020\n",
        "\n",
        "Mettre en √©vidence graphiquement l'impact du reconfinement.\n",
        "\n",
        "N'utiliser que les cat√©gories de titre _IMAGINE R_ et _Navigo_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oL5Asq--sm8"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNecvg2zbWE"
      },
      "source": [
        "#### Bonus\n",
        "\n",
        "Calculer la moyenne glissante sur 7 jours par categorie de titre pour r√©duire les variations hebdomadaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbdbg4dZ-0mI"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjTPspGbAecD"
      },
      "source": [
        "## Mod√©lisation avec Apache Spark\n",
        "\n",
        "On essaie de faire un mod√®le basique de pr√©vision du trafic dans les 7 prochains jours, pour une station.\n",
        "\n",
        "Apache Spark MLlib n'int√®gre pas de mod√®le pour les s√©ries chronologiques.\n",
        "\n",
        "L'approche classique est alors d'utiliser une technique de r√©gression classique (r√©gression lin√©aire bien s√ªr, mais aussi RandomForestRegressor par exemple).\n",
        "\n",
        "Pour une premi√®re version simple, utiliser un vecteur constituer des validations sur les 14 jours pr√©c√©dents (X) pour pr√©dire les validations du jour (y). Dans cette version, on utilisera une `LinearRegression` ou un `RandomForestRegressor`, au choix.\n",
        "\n",
        "Le code doit comporter :\n",
        "- la pr√©paration des _features_ (X)\n",
        "- la constitution d'un ensemble d'apprentissage et de test\n",
        "- l'entrainement d'un mod√®le\n",
        "- le mesure de la performance du mod√®le : RMSE\n",
        "\n",
        "Rappel : ne travailler que sur les deux cat√©gories de titre principales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HQKOiPQvVWN"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMKn6rmDREr"
      },
      "source": [
        "## Am√©lioration du mod√®le\n",
        "\n",
        "Discuter des fa√ßons d'am√©liorer cette premi√®re version du mod√®le.\n",
        "\n",
        "On pourra se reporter par exemple √† l'article suivant pour l'usage des for√™ts al√©atoires sur ce type de probl√®me : https://arxiv.org/abs/2101.02118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdse1jBVDcR-"
      },
      "source": [
        "**Remplir votre commentaire ici**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLJ6mndDkEk"
      },
      "source": [
        "## OPTION : impl√©mentation des am√©liorations\n",
        "\n",
        "Impl√©menter tout ou partir des suggestions d'am√©lioration.\n",
        "\n",
        "**Cette partie est facultative et sera bonifiante si les √©l√©ments contribu√©s sont probants.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlhwO0DEDM37"
      },
      "outputs": [],
      "source": [
        "# votre code ici"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
