{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF33cvquA22Y"
      },
      "source": [
        "# Mini-projet Apache Spark\n",
        "\n",
        "Merci d'indiquer les noms composant le binôme :\n",
        "\n",
        "|Nom | Prénom|\n",
        "|---|---|\n",
        "| A | compléter |\n",
        "| A | compléter |\n",
        "\n",
        "L'objectif est de collecter les données de validations quotidiennes de titres de transport de la région parisienne, disponibles en Open Data sur le site de Mobilités Ile-de-France.\n",
        "\n",
        "On se concentrera sur le **réseau ferré** exclusivement pour cet exercice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fXsbY1BRCt"
      },
      "source": [
        "## Installation de Spark ⚡\n",
        "\n",
        "Apache Spark n'est pas disponible en standard sur Google Colab.\n",
        "Procéder à son installation, ainsi qu'à son initialisation pour réaliser le traitement à venir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xk3QdgByAw-h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.9/site-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
            "Please visit http://www.java.com for information on installing Java.\n",
            "\n",
            "/opt/anaconda3/lib/python3.9/site-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
            "head: illegal line count -- -1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Java gateway process exited before sending its port number",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/Users/lilyhuong/Desktop/Amse mag3/Hadoop_spark/Traitement_des_validations_du_réseau_de_transport_dIle_de_France_2022.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lilyhuong/Desktop/Amse%20mag3/Hadoop_spark/Traitement_des_validations_du_r%C3%A9seau_de_transport_dIle_de_France_2022.ipynb#ch0000031?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39;49msql\u001b[39m.\u001b[39;49mSparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mProject 2022\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=266'>267</a>\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=267'>268</a>\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=268'>269</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=269'>270</a>\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=270'>271</a>\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py?line=271'>272</a>\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=480'>481</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=481'>482</a>\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=482'>483</a>\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=483'>484</a>\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=484'>485</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=188'>189</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=189'>190</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=190'>191</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=191'>192</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=192'>193</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=194'>195</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=195'>196</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=196'>197</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=197'>198</a>\u001b[0m         master,\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=198'>199</a>\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=207'>208</a>\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=208'>209</a>\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=414'>415</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=415'>416</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=416'>417</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=417'>418</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py?line=419'>420</a>\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=102'>103</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=104'>105</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=105'>106</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=107'>108</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    <a href='file:///opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py?line=108'>109</a>\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
          ]
        }
      ],
      "source": [
        "spark = pyspark.sql.SparkSession.builder.appName('Project 2022').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFkMo_2WBcrG"
      },
      "source": [
        "## Récupération des données 🚆\n",
        "\n",
        "Sur le site https://data.iledefrance-mobilites.fr, récupérer les données de validation par jour.\n",
        "\n",
        "On récupérera les fichiers des validations sur le réseau de surface disponibles ici [https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/table/](https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/table/). On prendra les années 2020 et 2021 pour l'exercice :\n",
        "* data-rf-2020.zip\n",
        "* data-rf-2021.zip\n",
        "\n",
        "Identifiez les données de S1 2020 à S2 2021 pour disposer d'un historique de deux ans.\n",
        "\n",
        "On utilisera pour ce faire les commandes de téléchargement de fichiers (`curl`) depuis un site (pas de chargement manuel).\n",
        "\n",
        "__Attention__ : prévoir quelques minutes pour le téléchargement, au moins une première fois, et donc une copie sur Google Drive si Google Colab est utilisée, afin d'éviter ce temps d'attente lors de sessions de travail successives.\n",
        "\n",
        "Les fichiers sont des zip. Ils peuvent être décompressés avec la commande `unzip`.\n",
        "\n",
        "Ces fichiers seront localisés dans un sous-répertoire.\n",
        "\n",
        "__Documentation de curl__ : http://pwet.fr/man/linux/commandes/curl/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6mVDDBi4PAWT"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JieIOSxp_8wM"
      },
      "source": [
        "### Commentaires sur les données disponibles sur ce portail\n",
        "\n",
        "Investiguer les données diponibles sur le portail.\n",
        "\n",
        "Question : peut-on constituer un historique de données s'étendant sur les trois dernières années (2019 à 2021) ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEXg6TeoATXD"
      },
      "source": [
        "**Remplir votre commentaire ici**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i0jSKz1Orfl"
      },
      "source": [
        "## Lecture des fichiers dans Spark 📁\n",
        "\n",
        "Lire les fichiers en choisissant les bonnes options de lecture.\n",
        "Concaténer les données en une seule table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Me1V5NfYx2A"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wPinQtj8vm7"
      },
      "source": [
        "## Préparation des données\n",
        "\n",
        "Réaliser les transformations nécessaires pour exploiter ces données :\n",
        "- préparation des dates (champ JOUR)\n",
        "\n",
        "Quelle est la tranformation à réaliser ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRzuTMKgO3Uw"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDTdkHk8lae"
      },
      "source": [
        "## Détermination des principales catégories de titre\n",
        "\n",
        "Différentes catégories de titre sont utilisées sur le réseau.\n",
        "\n",
        "Déterminer les deux catégories principalement utilisées. Seules ces catégories seront utilisées dans les travaux ci-après (les utiliser comme filtre sur les validations dans la suite)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGvfkzZTFiLn"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iY3-EDD6QfK"
      },
      "source": [
        "## Visualisation du trafic dans une station\n",
        "\n",
        "Visualiser le trafic à la gare de Lyon pour les deux catégories de titre principales.\n",
        "\n",
        "Attention à gérer le cas des gares (comme la gare de Lyon) présentes sur plusieurs lignes et dont le libellé apparaît donc sur plusieurs lignes. Investiguer ce cas avant de déterminer la bonne façon de calculer le nombre de validations pour la gare de Lyon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7kJok808_0H"
      },
      "outputs": [],
      "source": [
        "# Votre code mettant en évidence le cas des gares sur plusieurs lignes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeOqG1Hi9EuM"
      },
      "outputs": [],
      "source": [
        "# Votre code visualisant le trafic à la gare de Lyon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ae6sckJdxjE"
      },
      "source": [
        "## Fluctuation du trafic hebdomadaire\n",
        "\n",
        "Calculer le trafic total et le pourcentage par jour de la semaine sur l'ensemble du réseau.\n",
        "\n",
        "Trier le résultat par ordre décroissant de validations.\n",
        "\n",
        "Note : considérer l'usage d'une fonction analytique (`Window.partitionBy()`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku4EWiub-x4r"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvhcCQaJaB8i"
      },
      "source": [
        "### Impact du télétravail\n",
        "\n",
        "Mettre en évidence l'impact du télétravail en comparant les comportements hebdomadaires de janvier et février 2020 et ceux de janvier et février 2021."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqg42KydaQ7h"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMN5ZAs_dRWN"
      },
      "source": [
        "## Analyse de l'impact du reconfinement d'octobre 2020\n",
        "\n",
        "Mettre en évidence graphiquement l'impact du reconfinement.\n",
        "\n",
        "N'utiliser que les catégories de titre _IMAGINE R_ et _Navigo_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oL5Asq--sm8"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNecvg2zbWE"
      },
      "source": [
        "#### Bonus\n",
        "\n",
        "Calculer la moyenne glissante sur 7 jours par categorie de titre pour réduire les variations hebdomadaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbdbg4dZ-0mI"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjTPspGbAecD"
      },
      "source": [
        "## Modélisation avec Apache Spark\n",
        "\n",
        "On essaie de faire un modèle basique de prévision du trafic dans les 7 prochains jours, pour une station.\n",
        "\n",
        "Apache Spark MLlib n'intègre pas de modèle pour les séries chronologiques.\n",
        "\n",
        "L'approche classique est alors d'utiliser une technique de régression classique (régression linéaire bien sûr, mais aussi RandomForestRegressor par exemple).\n",
        "\n",
        "Pour une première version simple, utiliser un vecteur constituer des validations sur les 14 jours précédents (X) pour prédire les validations du jour (y). Dans cette version, on utilisera une `LinearRegression` ou un `RandomForestRegressor`, au choix.\n",
        "\n",
        "Le code doit comporter :\n",
        "- la préparation des _features_ (X)\n",
        "- la constitution d'un ensemble d'apprentissage et de test\n",
        "- l'entrainement d'un modèle\n",
        "- le mesure de la performance du modèle : RMSE\n",
        "\n",
        "Rappel : ne travailler que sur les deux catégories de titre principales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HQKOiPQvVWN"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMKn6rmDREr"
      },
      "source": [
        "## Amélioration du modèle\n",
        "\n",
        "Discuter des façons d'améliorer cette première version du modèle.\n",
        "\n",
        "On pourra se reporter par exemple à l'article suivant pour l'usage des forêts aléatoires sur ce type de problème : https://arxiv.org/abs/2101.02118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdse1jBVDcR-"
      },
      "source": [
        "**Remplir votre commentaire ici**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLJ6mndDkEk"
      },
      "source": [
        "## OPTION : implémentation des améliorations\n",
        "\n",
        "Implémenter tout ou partir des suggestions d'amélioration.\n",
        "\n",
        "**Cette partie est facultative et sera bonifiante si les éléments contribués sont probants.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlhwO0DEDM37"
      },
      "outputs": [],
      "source": [
        "# votre code ici"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
