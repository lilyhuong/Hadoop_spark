{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QvhcCQaJaB8i",
        "QYNecvg2zbWE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilyhuong/Hadoop_spark/blob/master/Traitement_des_validations_du_re%CC%81seau_de_transport_dIle_de_France_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF33cvquA22Y"
      },
      "source": [
        "# Mini-projet Apache Spark\n",
        "\n",
        "Merci d'indiquer les noms composant le bin√¥me :\n",
        "\n",
        "|Nom | Pr√©nom|\n",
        "|---|---|\n",
        "| Nguyen | Thi Huong |\n",
        "| Stary | Joffrey |\n",
        "\n",
        "L'objectif est de collecter les donn√©es de validations quotidiennes de titres de transport de la r√©gion parisienne, disponibles en Open Data sur le site de Mobilit√©s Ile-de-France.\n",
        "\n",
        "On se concentrera sur le **r√©seau ferr√©** exclusivement pour cet exercice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fXsbY1BRCt"
      },
      "source": [
        "## Installation de Spark ‚ö°\n",
        "\n",
        "Apache Spark n'est pas disponible en standard sur Google Colab.\n",
        "Proc√©der √† son installation, ainsi qu'√† son initialisation pour r√©aliser le traitement √† venir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3QdgByAw-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d0af36-8be5-45ac-ea5d-19e149b10e12"
      },
      "source": [
        "# Installation de pyspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 281.4 MB 49 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199 kB 63.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=36f53f14c8fe507866ebeb3a30e8713aada2203cd5bfc02912f8b041ae3f5d05\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation de pyspark\n",
        "import pyspark"
      ],
      "metadata": {
        "id": "Ia_0kHlxCIai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©ation de la session\n",
        "spark = pyspark.sql.SparkSession.builder.appName('Project 2022').getOrCreate()"
      ],
      "metadata": {
        "id": "-rcmRutJFupe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "BydbjmTwGPW5",
        "outputId": "d0f5e7e7-8b99-4b96-e5da-1f55d747d500"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7e1ffcb390>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2a3863ad20a4:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Project 2022</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFkMo_2WBcrG"
      },
      "source": [
        "## R√©cup√©ration des donn√©es üöÜ\n",
        "\n",
        "Sur le site https://data.iledefrance-mobilites.fr, r√©cup√©rer les donn√©es de validation par jour.\n",
        "\n",
        "On r√©cup√©rera les fichiers des validations sur le r√©seau de surface disponibles ici [https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/table/](https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/table/). On prendra les ann√©es 2020 et 2021 pour l'exercice :\n",
        "* data-rf-2020.zip\n",
        "* data-rf-2021.zip\n",
        "\n",
        "Identifiez les donn√©es de S1 2020 √† S2 2021 pour disposer d'un historique de deux ans.\n",
        "\n",
        "On utilisera pour ce faire les commandes de t√©l√©chargement de fichiers (`curl`) depuis un site (pas de chargement manuel).\n",
        "\n",
        "__Attention__ : pr√©voir quelques minutes pour le t√©l√©chargement, au moins une premi√®re fois, et donc une copie sur Google Drive si Google Colab est utilis√©e, afin d'√©viter ce temps d'attente lors de sessions de travail successives.\n",
        "\n",
        "Les fichiers sont des zip. Ils peuvent √™tre d√©compress√©s avec la commande `unzip`.\n",
        "\n",
        "Ces fichiers seront localis√©s dans un sous-r√©pertoire.\n",
        "\n",
        "__Documentation de curl__ : http://pwet.fr/man/linux/commandes/curl/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notre r√©ponse** \n",
        "\n",
        "Afin de t√©l√©charger les donn√©es, nous utilisons les liens disponibles sur le site d'√éle-de-France mobilit√©s. Les fichiers obtenus sont au format .zip ce qui n√©cessite de les unzip. Ce que nous faisons en reprenant les fonctions que vous nous conseillez. On obtient finalement les deux folders : ***data-rf-2020*** & ***data-rf-2021*** contenant les informations relatives √† la fr√©quentation mesur√©e par semestre.\n",
        "\n",
        "L'√©tude d'un document explicatif ([Pr√©sentation des donn√©es Open Data](https://data.iledefrance-mobilites.fr/api/datasets/1.0/histo-validations-reseau-ferre/attachments/donnees_de_validation_pdf/)) nous am√®ne √† consid√©rer ***NB_FER*** comme le jeu de donn√©es qui nous sera utile pour la suite du projet. Ce dernier nous renseigne sur le nombre de validation par jour, par cat√©gorie de titre et par station sur le r√©seau f√©rr√© √Ælien.\n",
        "\n",
        "Afin que les donn√©es restent disponibles au t√©l√©chargement, nous avons cr√©√© un d√©p√¥t sur ***github*** depuis lequel les donn√©es sont accessibles. Si la premi√®re partie du code ne fonctionne pas en raison d'une impossibilit√© √† t√©l√©charger depuis √éle-de-France mobilit√©s, alors utilisez la partie de code \"***Alternative***\"."
      ],
      "metadata": {
        "id": "6O_0filcqvHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T√©l√©chargement r√©pertoire 2020\n",
        "!curl -o file_2020.zip https://data.iledefrance-mobilites.fr/api/v2/catalog/datasets/histo-validations-reseau-ferre/files/e6bcf4c994951fc086e31db6819a3448\n",
        "# T√©l√©chargement r√©pertoire 2021\n",
        "!curl -o file_2021.zip https://data.iledefrance-mobilites.fr/api/v2/catalog/datasets/histo-validations-reseau-ferre/files/e35b9ec0a183a8f2c7a8537dd43b124c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72d_h7b1subi",
        "outputId": "d66dc80d-58c5-45fc-8752-5344d04c8029"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 82.8M    0 82.8M    0     0  31.9M      0 --:--:--  0:00:02 --:--:-- 31.9M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 16.0M    0 16.0M    0     0  29.4M      0 --:--:-- --:--:-- --:--:-- 29.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On d√©compresse les fichiers qui sont au format .zip\n",
        "  # Si le fichier existe d√©j√† alors on le remplace\n",
        "!unzip -o /content/file_2020.zip\n",
        "!unzip -o /content/file_2021.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRCXydMO2Cj7",
        "outputId": "47f01122-a7e4-43b6-e161-102ee8bccb25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/file_2020.zip\n",
            "  inflating: data-rf-2020/2020_S1_NB_FER.txt  \n",
            "  inflating: data-rf-2020/2020_S1_PROFIL_FER.txt  \n",
            "  inflating: data-rf-2020/2020_S2_NB_FER.txt  \n",
            "  inflating: data-rf-2020/2020_S2_PROFIL_FER.txt  \n",
            " extracting: data-rf-2015.zip        \n",
            " extracting: data-rf-2016.zip        \n",
            " extracting: data-rf-2017.zip        \n",
            " extracting: data-rf-2018.zip        \n",
            " extracting: data-rf-2019.zip        \n",
            "Archive:  /content/file_2021.zip\n",
            "  inflating: data-rf-2021/2021_S1_NB_FER.txt  \n",
            "  inflating: data-rf-2021/2021_S1_PROFIL_FER.txt  \n",
            "  inflating: data-rf-2021/2021_S2_NB_FER.txt  \n",
            "  inflating: data-rf-2021/2021_S2_PROFIL_FER.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "gFYR_dEmUgul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AtUK12bdU2TY",
        "outputId": "86ba26ab-0f65-4944-8e91-7438eb43edde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Am√©liorer avec une liste et v√©rifier que √ßa existe!!!!\n",
        "\n",
        "# Importation os\n",
        "import os\n",
        "\n",
        "# Mise au format .csv\n",
        "os.rename('/content/data-rf-2020/2020_S1_NB_FER.txt', '/content/data-rf-2020/2020_S1_NB_FER.csv')\n",
        "os.rename('/content/data-rf-2020/2020_S2_NB_FER.txt', '/content/data-rf-2020/2020_S2_NB_FER.csv')\n",
        "os.rename('/content/data-rf-2021/2021_S1_NB_FER.txt', '/content/data-rf-2021/2021_S1_NB_FER.csv')\n",
        "os.rename('/content/data-rf-2021/2021_S2_NB_FER.txt', '/content/data-rf-2021/2021_S2_NB_FER.csv')"
      ],
      "metadata": {
        "id": "vM_MYsKmD8QT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Alternative en cas d'inaccessibilit√© des donn√©es** ‚õî\n",
        "\n",
        "Le code qui suit permet de t√©l√©charger les jeux de donn√©es si ils n'√©taient pas disponible sur √éle-de-France mobilit√©.\n",
        "\n",
        "**Ne faire tourner que si n√©cessaire. ‚ùó**"
      ],
      "metadata": {
        "id": "S7mYGv45B5zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T√©l√©chargement des jeux de donn√©es depuis un github h√©bergeant les jeux de donn√©es pour les 4 semestres au format .csv\n",
        "  # Ces fichiers ont √©t√© obtenus initiallement depuis la partie pr√©c√©dent du code et une extraction manuelle depuis google collab vers le github\n",
        "\n",
        "!curl -o S1_2020 https://raw.githubusercontent.com/lilyhuong/Hadoop_spark/master/2020_S1_NB_FER.csv\n",
        "!curl -o S2_2020 https://raw.githubusercontent.com/lilyhuong/Hadoop_spark/master/2020_S2_NB_FER.csv\n",
        "!curl -o S1_2021 https://raw.githubusercontent.com/lilyhuong/Hadoop_spark/master/2021_S1_NB_FER.csv\n",
        "!curl -o S2_2021 https://raw.githubusercontent.com/lilyhuong/Hadoop_spark/master/2021_S2_NB_FER.csv"
      ],
      "metadata": {
        "id": "mMdUfNDcu7Gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b49a04f-9bbc-4d01-dea8-ab53c5e62a48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 47.8M  100 47.8M    0     0  20.5M      0  0:00:02  0:00:02 --:--:-- 20.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 56.8M  100 56.8M    0     0  20.9M      0  0:00:02  0:00:02 --:--:-- 20.9M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 57.5M  100 57.5M    0     0  19.2M      0  0:00:02  0:00:02 --:--:-- 19.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 58.7M  100 58.7M    0     0  18.9M      0  0:00:03  0:00:03 --:--:-- 18.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mVDDBi4PAWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e181ef25-1ead-419e-8b67-41e4cbdb969c"
      },
      "source": [
        "!curl -O https://raw.githubusercontent.com/lilyhuong/Hadoop_spark/master/2020_S1_NB_FER.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 47.8M  100 47.8M    0     0  3725k      0  0:00:13  0:00:13 --:--:-- 12.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/lilyhuong/Hadoop_spark/master/2021_S2_NB_FER.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpRT8qJVM08t",
        "outputId": "e51868fd-b000-4c85-e04a-75bcc2ae721f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 58.7M  100 58.7M    0     0  20.0M      0  0:00:02  0:00:02 --:--:-- 20.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head 2021_S2_NB_FER.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3sZXQGOBw1",
        "outputId": "ad83d9c1-784a-4992-a694-bc7fe5ebf369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JOUR\tCODE_STIF_TRNS\tCODE_STIF_RES\tCODE_STIF_ARRET\tLIBELLE_ARRET\tID_REFA_LDA\tCATEGORIE_TITRE\tNB_VALD\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\t?\t204\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tAMETHYSTE\t157\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tAUTRE TITRE\t983\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tFGT\t304\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tIMAGINE R\t1958\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tNAVIGO\t8821\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tNAVIGO JOUR\t43\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tNON DEFINI\t766\r\n",
            "01/07/2021\t100\t110\t1\tPORTE MAILLOT\t71379\tTST\t613\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JieIOSxp_8wM"
      },
      "source": [
        "### Commentaires sur les donn√©es disponibles sur ce portail\n",
        "\n",
        "Investiguer les donn√©es diponibles sur le portail.\n",
        "\n",
        "Question : peut-on constituer un historique de donn√©es s'√©tendant sur les trois derni√®res ann√©es (2019 √† 2021) ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEXg6TeoATXD"
      },
      "source": [
        "## **Notre r√©ponse**\n",
        "\n",
        "Le portail √éle-de-France mobilit√©s offre des donn√©es sur la fr√©quentation mesur√©e du r√©seau ferr√© pour la p√©riode 2015-2021 (voir : [Historique des donn√©es de validation sur le r√©seau ferr√© (2015-2021)](https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B)). La structure des fichiers est la m√™me pour toutes ces ann√©es ce qui rend l'ajout de l'ann√©e 2019 dans l'historique simple, il suffit d'ajouter le t√©l√©chargement et l'extraction du jeu de donn√©es pour 2019 pour √©tablir cet historique. La m√©thode reste similaire √† ce qui a √©t√© fait pr√©c√©demment pour 2020 et 2021. Il en est de m√™me pour toute la p√©riode couverte par ce jeu de donn√©es (soit 2015-2021 en Novembre 2022).\n",
        "\n",
        "***La cr√©ation d'une historique pour la p√©riode 2019-2021 est donc possible et impl√©mentable sans difficult√©s.***\n",
        "\n",
        "On peut d'ailleurs ajouter que les donn√©es sont mises √† jour √† un rythme bi-annuel (derni√®re mise √† jour en octobre 2022 - [source](https://data.iledefrance-mobilites.fr/explore/dataset/histo-validations-reseau-ferre/information/)). Il serait donc possible de continuellement suivre cet historique avec la mise √† jour des donn√©es en OPEN DATA, √† condition que la politique de diffusion des donn√©es et leur nature ne soit pas alt√©r√©s.\n",
        "\n",
        "Ci-dessous le lien pour t√©l√©charger les donn√©es de 2019 :\n",
        "\n",
        "> [Donn√©es pour 2019](https://data.iledefrance-mobilites.fr/api/v2/catalog/datasets/histo-validations-reseau-ferre/files/6d7e7b859e6acac7bebad18bdb37bfc3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i0jSKz1Orfl"
      },
      "source": [
        "## Lecture des fichiers dans Spark üìÅ\n",
        "\n",
        "Lire les fichiers en choisissant les bonnes options de lecture.\n",
        "Concat√©ner les donn√©es en une seule table."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notre r√©ponse**\n",
        "\n",
        "Les fichiers ont √©t√© pr√©alablement pass√©s au format .csv o√π la ***tabulation*** ('\\t') sert de s√©parateur. Le nom des variables est quant √† lui pr√©sent en header.\n",
        "\n",
        "Nous cr√©ons ***un dataframe par semestre***. En affichant les 5 premi√®res lignes de ces dataframes, on voit que la structure, les variables pr√©sentes sont similaires.\n",
        "\n",
        "On ***concat√®ne*** ensuite les dataframes ***par ann√©e*** pour obtenir un dataframe par ann√©e avant de le faire pour la p√©riode avec ***df_validations***, le dataframe ***pour toute la p√©riode*** qui sera utilis√© pour la suite de l'analyse."
      ],
      "metadata": {
        "id": "CRoIEd44v_IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©ation d'un dataframe par semestre entre S1 2020 et S2 2021\n",
        "df_2020_S1 = spark.read.csv('/content/data-rf-2020/2020_S1_NB_FER.csv', sep = '\\t', header = True)\n",
        "df_2020_S2 = spark.read.csv('/content/data-rf-2020/2020_S2_NB_FER.csv', sep = '\\t', header = True)\n",
        "df_2021_S1 = spark.read.csv('/content/data-rf-2021/2021_S1_NB_FER.csv', sep = '\\t', header = True)\n",
        "df_2021_S2 = spark.read.csv('/content/data-rf-2021/2021_S2_NB_FER.csv', sep = '\\t', header = True)"
      ],
      "metadata": {
        "id": "vc7y0BnfCb4Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2020_S1 = spark.read.text('/content/data-rf-2020/2020_S1_NB_FER.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "znLzyXaUVqQK",
        "outputId": "cb762e22-978c-4092-c7d2-48351bdd0404"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e962d47cd493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_2020_S1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data-rf-2020/2020_S1_NB_FER.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     def csv(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/data-rf-2020/2020_S1_NB_FER.txt"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les t√™tes des data frames (par semestre)\n",
        "df_2020_S1.show(5)\n",
        "df_2020_S2.show(5)\n",
        "df_2021_S1.show(5)\n",
        "df_2021_S2.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjDBBVMjMJ-e",
        "outputId": "34bc5093-0f8b-44dc-bf76-d4b0bd1eb9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|              ?|     28|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|      AMETHYSTE|     83|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|    AUTRE TITRE|     94|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|            FGT|     84|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|      IMAGINE R|    522|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|01/07/2020|           100|          110|              1|PORTE MAILLOT|      71379|              ?|     99|\n",
            "|01/07/2020|           100|          110|              1|PORTE MAILLOT|      71379|      AMETHYSTE|    102|\n",
            "|01/07/2020|           100|          110|              1|PORTE MAILLOT|      71379|    AUTRE TITRE|    273|\n",
            "|01/07/2020|           100|          110|              1|PORTE MAILLOT|      71379|            FGT|    225|\n",
            "|01/07/2020|           100|          110|              1|PORTE MAILLOT|      71379|      IMAGINE R|   1285|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|              ?|     45|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|      AMETHYSTE|     52|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|    AUTRE TITRE|     50|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|            FGT|     81|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|      IMAGINE R|    369|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|01/07/2021|           100|          110|              1|PORTE MAILLOT|      71379|              ?|    204|\n",
            "|01/07/2021|           100|          110|              1|PORTE MAILLOT|      71379|      AMETHYSTE|    157|\n",
            "|01/07/2021|           100|          110|              1|PORTE MAILLOT|      71379|    AUTRE TITRE|    983|\n",
            "|01/07/2021|           100|          110|              1|PORTE MAILLOT|      71379|            FGT|    304|\n",
            "|01/07/2021|           100|          110|              1|PORTE MAILLOT|      71379|      IMAGINE R|   1958|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concat√©nations des donn√©es par ann√©e\n",
        "df_2020 = df_2020_S1.unionByName(df_2020_S2)\n",
        "df_2021 = df_2021_S1.unionByName(df_2021_S2)\n",
        "\n",
        "# Concat√©nations des donn√©es sur la p√©riode\n",
        "df_validations = df_2020.unionByName(df_2021)"
      ],
      "metadata": {
        "id": "8g5H51Y8NfXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les t√™tes des data frames (par ann√©e)\n",
        "df_2020.show(10)\n",
        "df_2021.show(10)\n",
        "\n",
        "# Afficher les t√™tes des data frames (sur la p√©riode)\n",
        "df_validations.show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAIehVSIO1XV",
        "outputId": "debf7ada-0480-4a7e-92f7-39ccf4e60b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|              ?|     28|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|      AMETHYSTE|     83|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|    AUTRE TITRE|     94|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|            FGT|     84|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|      IMAGINE R|    522|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|         NAVIGO|   2049|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|    NAVIGO JOUR|      5|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|     NON DEFINI|     72|\n",
            "|01/01/2020|           100|          110|              1|PORTE MAILLOT|      71379|            TST|     92|\n",
            "|01/01/2020|           100|          110|           1006|   OLYMPIADES|      71557|              ?|     23|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|              ?|     45|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|      AMETHYSTE|     52|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|    AUTRE TITRE|     50|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|            FGT|     81|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|      IMAGINE R|    369|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|         NAVIGO|    848|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|    NAVIGO JOUR|      5|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|     NON DEFINI|    132|\n",
            "|01/01/2021|           100|          110|              1|PORTE MAILLOT|      71379|            TST|    101|\n",
            "|01/01/2021|           100|          110|             10|       ALESIA|      71030|              ?|     26|\n",
            "+----------+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n",
            "|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|       LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|              ?|     28|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|      AMETHYSTE|     83|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|    AUTRE TITRE|     94|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|            FGT|     84|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|      IMAGINE R|    522|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|         NAVIGO|   2049|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|    NAVIGO JOUR|      5|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|     NON DEFINI|     72|\n",
            "|01/01/2020|           100|          110|              1|       PORTE MAILLOT|      71379|            TST|     92|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|              ?|     23|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|      AMETHYSTE|    161|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|    AUTRE TITRE|     82|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|            FGT|    160|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|      IMAGINE R|    788|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|         NAVIGO|   2207|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|    NAVIGO JOUR|      5|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|     NON DEFINI|    134|\n",
            "|01/01/2020|           100|          110|           1006|          OLYMPIADES|      71557|            TST|    205|\n",
            "|01/01/2020|           100|          110|           1007|LES AGNETTES-ASNI...|      72240|              ?|     10|\n",
            "|01/01/2020|           100|          110|           1007|LES AGNETTES-ASNI...|      72240|      AMETHYSTE|     10|\n",
            "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pr√©paration des donn√©es\n",
        "\n",
        "R√©aliser les transformations n√©cessaires pour exploiter ces donn√©es :\n",
        "- pr√©paration des dates (champ JOUR)\n",
        "\n",
        "Quelle est la tranformation √† r√©aliser ?"
      ],
      "metadata": {
        "id": "2wPinQtj8vm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notre r√©ponse**\n",
        "\n",
        "TO WRITE Expliquer la d√©marche"
      ],
      "metadata": {
        "id": "6UXxkXpzQMkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Probablement inutile de traiter autre chose que JOUR\n",
        "    # On ne se sert pas trop des autres vars\n",
        "    # Le point ? c'est normal\n",
        "  \n",
        "  # De Joffrey\n",
        "    # Je pense qu'il faut UNIQUEMENT traiter JOUR\n",
        "      # Bon format,\n",
        "      # V√©rifier si miss\n",
        "      # V√©rifier si outliers (des jours hors de la p√©riode 1er janvier 2020 √† 31 d√©cembre 2021)\n",
        "\n",
        "  # Mais par la suite, besoin de quelques traitements..."
      ],
      "metadata": {
        "id": "GiYMBIczPYOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va regarder la structure de dataframe "
      ],
      "metadata": {
        "id": "ZRMUiDLnbOpq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRzuTMKgO3Uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9062f340-01e4-4602-98f4-ec3ecc4f78f9"
      },
      "source": [
        "# Affichage de la structure du dataframe\n",
        "df_validations.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- JOUR: string (nullable = true)\n",
            " |-- CODE_STIF_TRNS: string (nullable = true)\n",
            " |-- CODE_STIF_RES: string (nullable = true)\n",
            " |-- CODE_STIF_ARRET: string (nullable = true)\n",
            " |-- LIBELLE_ARRET: string (nullable = true)\n",
            " |-- ID_REFA_LDA: string (nullable = true)\n",
            " |-- CATEGORIE_TITRE: string (nullable = true)\n",
            " |-- NB_VALD: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pour la variable jour, on doit convert en type datetime, NB_VALD to numeric \n",
        "* Check les valeurs manquants pour chaque variable => chercher une solution\n",
        "* Faire une descriptive statistic pour v√©rifier si il y a des outliers dans notre base de donn√©e (notatement dans la colonne NB VALID\n"
      ],
      "metadata": {
        "id": "c9KhFqUZeG50"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wX9_Xs22eF6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pr√©paration des donn√©es**\n",
        "\n",
        "En plus da la variable ***JOUR***, nous pr√©parons d√®s √† pr√©sent certaines variables.\n",
        "\n",
        "\n",
        "\n",
        "*   ***Valeurs manquantes***\n",
        "  *   On regarde pour toutes les varibles si il y a des valeurs manquantes.\n",
        "\n",
        "*   ***NB_VALD***\n",
        "  *   Cette variable donne le nombre de validations ce qui implique d'en faire une variable num√©rique (enti√®re car il ne peut pas y avoir de parti de validations, un passage est valid√© ou non).\n",
        "  *   Solution - Passage en ***integer***.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vq_3304N1Pto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ne marche pas √ßa reste string, revoir\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "G1qRoVkl3HBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On regarde si il y a des valeurs manquantes\n",
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "\n",
        "df_validations.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_validations.columns]\n",
        "   ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdz_4VTF4IN5",
        "outputId": "11d16792-d5b9-488e-b9e5-165092177ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n",
            "+----+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "|   0|             0|            0|              0|            0|          0|              0|      0|\n",
            "+----+--------------+-------------+---------------+-------------+-----------+---------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passer NB_VALD en integer\n",
        "#from pyspark.sql.functions import withColumn\n",
        "\n",
        "df_validations = df_validations.withColumn(\"NB_VALD\", df_validations.NB_VALD.cast('int'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zMhebQz92nZT",
        "outputId": "46911e92-8eef-45c4-fba8-58b19b6e258e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-64a69d701691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Passager integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwithColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_validations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_validations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NB_VALD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_validations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNB_VALD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'withColumn' from 'pyspark.sql.functions' (/usr/local/lib/python3.7/dist-packages/pyspark/sql/functions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unnXwTcy3BDn",
        "outputId": "2061ee71-6516-43ef-b2c6-c46c98e0fcca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- JOUR: string (nullable = true)\n",
            " |-- CODE_STIF_TRNS: string (nullable = true)\n",
            " |-- CODE_STIF_RES: string (nullable = true)\n",
            " |-- CODE_STIF_ARRET: string (nullable = true)\n",
            " |-- LIBELLE_ARRET: string (nullable = true)\n",
            " |-- ID_REFA_LDA: string (nullable = true)\n",
            " |-- CATEGORIE_TITRE: string (nullable = true)\n",
            " |-- NB_VALD: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDTdkHk8lae"
      },
      "source": [
        "## D√©termination des principales cat√©gories de titre\n",
        "\n",
        "Diff√©rentes cat√©gories de titre sont utilis√©es sur le r√©seau.\n",
        "\n",
        "D√©terminer les deux cat√©gories principalement utilis√©es. Seules ces cat√©gories seront utilis√©es dans les travaux ci-apr√®s (les utiliser comme filtre sur les validations dans la suite)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notre r√©ponse**\n",
        "\n",
        "ffff"
      ],
      "metadata": {
        "id": "_NfAth31zacw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code ici"
      ],
      "metadata": {
        "id": "oGvfkzZTFiLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Do List - Joffrey\n",
        "  # CATEGORIE_TITRE\n",
        "    # Liste des cat√©gories\n",
        "    # Trouver les 2 cat√©gories qui reviennent le plus\n",
        "  # R√©utiliser ces 2 cat√©gories plus la SUITE\n",
        "    # Cr√©er un df avec uniquement les 2 cat√©gories les plus utilis√©es"
      ],
      "metadata": {
        "id": "Q6W3U10hDYRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation des fonctions SQL pyspark\n",
        "import pyspark.sql.functions\n",
        "\n",
        "df_validations.groupby('CATEGORIE_TITRE')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK9Dctndq57w",
        "outputId": "c73a6052-6767-487e-e4cb-b3dacfad69d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.group.GroupedData object at 0x7f9be915e910>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iY3-EDD6QfK"
      },
      "source": [
        "## Visualisation du trafic dans une station\n",
        "\n",
        "Visualiser le trafic √† la gare de Lyon pour les deux cat√©gories de titre principales.\n",
        "\n",
        "Attention √† g√©rer le cas des gares (comme la gare de Lyon) pr√©sentes sur plusieurs lignes et dont le libell√© appara√Æt donc sur plusieurs lignes. Investiguer ce cas avant de d√©terminer la bonne fa√ßon de calculer le nombre de validations pour la gare de Lyon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7kJok808_0H"
      },
      "source": [
        "# Votre code mettant en √©vidence le cas des gares sur plusieurs lignes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LiLy\n",
        "  \n",
        "  # Un m√™me lieu (gare) est pr√©sent sur plusieurs lignes (plusieurs libell√©s)\n",
        "  # Attention √† bien le faire par ann√©e et semestre\n",
        "  # https://data.iledefrance-mobilites.fr/api/datasets/1.0/histo-validations-reseau-ferre/attachments/donnees_de_validation_pdf/\n",
        "\n",
        "# Etape 1\n",
        "  # Montrer pour un libell√© (gare de lyon) qu'il existe diff√©rents codes d'arr√™ts\n",
        "\n",
        "# Etape 2\n",
        "  # On veut additionner les validation pour :\n",
        "    # une m√™me cat√©gorie\n",
        "    # CODE_STIF_ARRET diff√©rent mais m√™me LIBELLE_ARRET & m√™me jour\n",
        "    # ==> Groupby NBvalidation par cat√©gorie/jour/libelle d'arr√™t"
      ],
      "metadata": {
        "id": "XMe2ip0HEKC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeOqG1Hi9EuM"
      },
      "source": [
        "# Votre code visualisant le trafic √† la gare de Lyon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LiLy\n",
        "  \n",
        "  # Graphique (pandas, matplotlib), difficile sur spark (impossible ?)"
      ],
      "metadata": {
        "id": "-E7Q9xSMIU_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ae6sckJdxjE"
      },
      "source": [
        "## Fluctuation du trafic hebdomadaire\n",
        "\n",
        "Calculer le trafic total et le pourcentage par jour de la semaine sur l'ensemble du r√©seau.\n",
        "\n",
        "Trier le r√©sultat par ordre d√©croissant de validations.\n",
        "\n",
        "Note : consid√©rer l'usage d'une fonction analytique (`Window.partitionBy()`).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code ici"
      ],
      "metadata": {
        "id": "ku4EWiub-x4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joffrey\n",
        "\n",
        "# Etape 1\n",
        "  # Cr√©er une variable (1 √† 7) renvoyant √† un jour de la semaine (fonction dayofweek)\n",
        "  # L'attribuer √† toutes nos observations\n",
        "\n",
        "# Etape 2\n",
        "  # SUM les validations (2 cat√©gories r√©unies) pour une journ√©e, la somme des validations pour le lundi par exemple\n",
        "  # % des validations d'une journ√©e par rapport √† la semaine (Lundi/semaine)\n",
        "  # Trier par ordre d√©croissant\n",
        "  # graphique ?\n",
        "  \n",
        "  # SUM des validations par jour (Lundi √† Dimanche)\n",
        "  # % des validations d'une journ√©e ramen√©e √† la semaine"
      ],
      "metadata": {
        "id": "zoWe_t4xIiKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Impact du t√©l√©travail\n",
        "\n",
        "Mettre en √©vidence l'impact du t√©l√©travail en comparant les comportements hebdomadaires de janvier et f√©vrier 2020 et ceux de janvier et f√©vrier 2021."
      ],
      "metadata": {
        "id": "QvhcCQaJaB8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code ici"
      ],
      "metadata": {
        "id": "Gqg42KydaQ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Descriptive puis graphique\n",
        "  # 2 cat√©gories r√©unies ??\n",
        "\n",
        "  # Etape 1\n",
        "    # Comparer les 2 p√©riodes"
      ],
      "metadata": {
        "id": "A-X2t7HKLgMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMN5ZAs_dRWN"
      },
      "source": [
        "## Analyse de l'impact du reconfinement d'octobre 2020\n",
        "\n",
        "Mettre en √©vidence graphiquement l'impact du reconfinement.\n",
        "\n",
        "N'utiliser que les cat√©gories de titre _IMAGINE R_ et _Navigo_.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code ici"
      ],
      "metadata": {
        "id": "_oL5Asq--sm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Graphique avec une barre au moment du reconfinement + mettre 2021 pour comparer ?\n",
        "  # Plusieurs graphiques\n",
        "    # fusionn√©s\n",
        "    # courbes\n",
        "    # barplot\n",
        "    # cumul√© (surface rouge cat 1 et au dessus en bleu la cat 2)"
      ],
      "metadata": {
        "id": "gt5OeKZDL3pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNecvg2zbWE"
      },
      "source": [
        "#### Bonus\n",
        "\n",
        "Calculer la moyenne glissante sur 7 jours par categorie de titre pour r√©duire les variations hebdomadaires."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Votre code ici"
      ],
      "metadata": {
        "id": "qbdbg4dZ-0mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Et repr√©senter graphiquement\n",
        "  # Sur TOUTE la p√©riode"
      ],
      "metadata": {
        "id": "uahf9gRXM7Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjTPspGbAecD"
      },
      "source": [
        "## Mod√©lisation avec Apache Spark\n",
        "\n",
        "On essaie de faire un mod√®le basique de pr√©vision du trafic dans les 7 prochains jours, pour une station.\n",
        "\n",
        "Apache Spark MLlib n'int√®gre pas de mod√®le pour les s√©ries chronologiques.\n",
        "\n",
        "L'approche classique est alors d'utiliser une technique de r√©gression classique (r√©gression lin√©aire bien s√ªr, mais aussi RandomForestRegressor par exemple).\n",
        "\n",
        "Pour une premi√®re version simple, utiliser un vecteur constituer des validations sur les 14 jours pr√©c√©dents (X) pour pr√©dire les validations du jour (y). Dans cette version, on utilisera une `LinearRegression` ou un `RandomForestRegressor`, au choix.\n",
        "\n",
        "Le code doit comporter :\n",
        "- la pr√©paration des _features_ (X)\n",
        "- la constitution d'un ensemble d'apprentissage et de test\n",
        "- l'entrainement d'un mod√®le\n",
        "- le mesure de la performance du mod√®le : RMSE\n",
        "\n",
        "Rappel : ne travailler que sur les deux cat√©gories de titre principales.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQKOiPQvVWN"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMKn6rmDREr"
      },
      "source": [
        "## Am√©lioration du mod√®le\n",
        "\n",
        "Discuter des fa√ßons d'am√©liorer cette premi√®re version du mod√®le.\n",
        "\n",
        "On pourra se reporter par exemple √† l'article suivant pour l'usage des for√™ts al√©atoires sur ce type de probl√®me : https://arxiv.org/abs/2101.02118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdse1jBVDcR-"
      },
      "source": [
        "**Remplir votre commentaire ici**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLJ6mndDkEk"
      },
      "source": [
        "## OPTION : impl√©mentation des am√©liorations\n",
        "\n",
        "Impl√©menter tout ou partir des suggestions d'am√©lioration.\n",
        "\n",
        "**Cette partie est facultative et sera bonifiante si les √©l√©ments contribu√©s sont probants.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlhwO0DEDM37"
      },
      "source": [
        "# votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}